# -*- coding: utf-8 -*-
"""Arkavidia9_Datavidia_Susu Beruang

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VW47aEWiEneTdGT8gqcPJlZ_m3PXjOsM

#PRE PROCESSING
"""

import urllib.request
import zipfile
import pandas as pd
import tensorflow as tf
import math
import os
import tempfile

import pandas as pd
from torch.optim import AdamW

attribute = pd.read_csv('/content/Attributes (Imputed by Interpolation).csv')

# First, convert the 'Date' column to datetime objects using the correct format
attribute['Date'] = pd.to_datetime(attribute['Date'], format='%d/%m/%Y')

# Then, change the format to 'YYYY-MM-DD'
attribute['Date'] = attribute['Date'].dt.strftime('%Y-%m-%d')
attribute['Date'] = pd.to_datetime(attribute['Date'])

dataset_bawang_merah = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Bawang Merah.csv')
dataset_bawang_putih = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Bawang Putih Bonggol.csv')
dataset_beras_medium = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Beras Medium.csv')
dataset_beras_premium = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Beras Premium.csv')
dataset_cabai_merah = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Cabai Merah Keriting.csv')
dataset_cabai_rawit = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Cabai Rawit Merah.csv')
dataset_daging_ayam = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Daging Ayam Ras.csv')
dataset_daging_sapi = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Daging Sapi Murni.csv')
dataset_gula = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Gula Konsumsi.csv')
dataset_minyak_curah = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Minyak Goreng Curah.csv')
dataset_goreng_kemasan = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Minyak Goreng Kemasan Sederhana.csv')
dataset_telur = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Telur Ayam Ras.csv')
dataset_terigu = pd.read_csv('/content/arkavidia/Harga Bahan Pangan/train/Tepung Terigu (Curah).csv')

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_bawang_merah.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_bawang_merah[col] = impute_missing_values(dataset_bawang_merah[col])

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_bawang_putih.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_bawang_putih[col] = impute_missing_values(dataset_bawang_putih[col])

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_beras_medium.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_beras_medium[col] = impute_missing_values(dataset_beras_medium[col])

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_beras_premium.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_beras_premium[col] = impute_missing_values(dataset_beras_premium[col])

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_cabai_merah.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_cabai_merah[col] = impute_missing_values(dataset_cabai_merah[col])

dataset_cabai_merah.to_csv('dataset_cabai_merah.csv', index=False)

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_cabai_rawit.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_cabai_rawit[col] = impute_missing_values(dataset_cabai_rawit[col])

dataset_cabai_rawit.to_csv('dataset_cabai_rawit.csv', index=False)

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_daging_ayam.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_daging_ayam[col] = impute_missing_values(dataset_daging_ayam[col])

dataset_daging_ayam.to_csv('dataset_daging_ayam.csv', index=False)

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_daging_sapi.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_daging_sapi[col] = impute_missing_values(dataset_daging_sapi[col])

dataset_daging_sapi.to_csv('dataset_daging_sapi.csv', index=False)

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_gula.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_gula[col] = impute_missing_values(dataset_gula[col])

dataset_gula.to_csv('dataset_gula.csv', index=False)

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_minyak_curah.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_minyak_curah[col] = impute_missing_values(dataset_minyak_curah[col])

dataset_minyak_curah.to_csv('dataset_minyak_curah.csv', index=False)

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_goreng_kemasan.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_goreng_kemasan[col] = impute_missing_values(dataset_goreng_kemasan[col])

dataset_goreng_kemasan.to_csv('dataset_goreng_kemasan.csv', index=False)

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_telur.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_telur[col] = impute_missing_values(dataset_telur[col])

dataset_telur.to_csv('dataset_telur.csv', index=False)

import pymc as pm
import numpy as np
def impute_missing_values(series):
    with pm.Model():
        mu = pm.Normal("mu", mu=series.mean(), sigma=series.std())
        sigma = pm.HalfNormal("sigma", sigma=series.std())
        observed = pm.Normal("observed", mu=mu, sigma=sigma, observed=series.dropna())

        trace = pm.sample(1000, return_inferencedata=False)

        missing_indices = series[series.isnull()].index
        series.loc[missing_indices] = np.random.normal(trace["mu"].mean(), trace["sigma"].mean(), size=len(missing_indices))

    return series

# Imputasi missing values pada semua target kolom
target_columns = target_columns = dataset_terigu.drop(columns=['Date']).columns.tolist()
for col in target_columns:
    dataset_terigu[col] = impute_missing_values(dataset_terigu[col])

dataset_terigu.to_csv('dataset_terigu.csv', index=False)

bawang_merah = pd.concat([dataset_bawang_merah, attribute], ignore_index=True)
bawang_putih = pd.concat([dataset_bawang_putih, attribute], ignore_index=True)
beras_medium = pd.concat([dataset_beras_medium, attribute], ignore_index=True)
beras_premium = pd.concat([dataset_beras_premium, attribute], ignore_index=True)
cabai_merah = pd.concat([dataset_cabai_merah, attribute], ignore_index=True)
cabai_rawit = pd.concat([dataset_cabai_rawit, attribute], ignore_index=True)
daging_ayam = pd.concat([dataset_daging_ayam, attribute], ignore_index=True)
daging_sapi = pd.concat([dataset_daging_sapi, attribute], ignore_index=True)
gula = pd.concat([dataset_gula, attribute], ignore_index=True)
minyak_curah = pd.concat([dataset_minyak_curah, attribute], ignore_index=True)
goreng_kemasan = pd.concat([dataset_goreng_kemasan, attribute], ignore_index=True)
telur = pd.concat([dataset_telur, attribute], ignore_index=True)
terigu = pd.concat([dataset_terigu, attribute], ignore_index=True)

"""#EDA"""

from prophet import Prophet
from prophet.plot import plot_plotly
import plotly.offline as py
py.init_notebook_mode()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('fivethirtyeight')

df = pd.read_csv("dataframe (w_o test).csv")
df = df[['Date', 'Aceh']]

df.rename(columns = {'Aceh':'Aceh'}, inplace = True)df.rename(columns = {'Aceh':'Aceh'}, inplace = True)

df.info()

df['Date'] = pd.DatetimeIndex(df['Date'])
df.dtypes

ax = df.set_index('ds').plot(figsize=(12, 8))
ax.set_ylabel('Harga Cabai Merah Keriting')
ax.set_xlabel('Date')

plt.show()

"""#MODELLING"""

!pip install arch

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller, acf, pacf
from statsmodels.tsa.arima.model import ARIMA
from arch import arch_model
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error

# Load the data
data = pd.read_csv("gula.csv")

# Menentukan urutan kolom yang diinginkan
ordered_columns = [
    "Date", "USDIDR Close", "Price Crude Oil", "Change % Crude Oil", "Price Natural Gas", "Change % Natural Gas",
    "Price Newcastle Coal", "Change % Newcastle Coal", "Price Palm Oil", "Change % Palm Oil", "Price US Sugar",
    "Change % US Sugar", "Price US Wheat", "Change % US Wheat", "Aceh", "Bali", "Banten", "Bengkulu",
    "DI Yogyakarta", "DKI Jakarta", "Gorontalo", "Jambi", "Jawa Barat", "Jawa Tengah", "Jawa Timur",
    "Kalimantan Barat", "Kalimantan Selatan", "Kalimantan Tengah", "Kalimantan Timur", "Kalimantan Utara",
    "Kepulauan Bangka Belitung", "Kepulauan Riau", "Lampung", "Maluku Utara", "Maluku", "Nusa Tenggara Barat",
    "Nusa Tenggara Timur", "Papua Barat", "Papua", "Riau", "Sulawesi Barat", "Sulawesi Selatan", "Sulawesi Tengah",
    "Sulawesi Tenggara", "Sulawesi Utara", "Sumatera Barat", "Sumatera Selatan", "Sumatera Utara"
]
# Menyesuaikan DataFrame dengan urutan baru
data = data[ordered_columns]

data['Date'] = pd.to_datetime(data['Date']) # Change the format to '%d/%m/%Y'
data.set_index('Date', inplace=True)

# Extract time features
data['year'] = data.index.year
data['month'] = data.index.month
data['day'] = data.index.day
data['dayofweek'] = data.index.dayofweek

# Define features and target variables
target_columns = ['Aceh', 'Bali', 'Banten', 'Bengkulu',
       'DI Yogyakarta', 'DKI Jakarta', 'Gorontalo', 'Jambi', 'Jawa Barat',
       'Jawa Tengah', 'Jawa Timur', 'Kalimantan Barat', 'Kalimantan Selatan',
       'Kalimantan Tengah', 'Kalimantan Timur', 'Kalimantan Utara',
       'Kepulauan Bangka Belitung', 'Kepulauan Riau', 'Lampung',
       'Maluku Utara', 'Maluku', 'Nusa Tenggara Barat', 'Nusa Tenggara Timur',
       'Papua Barat', 'Papua', 'Riau', 'Sulawesi Barat', 'Sulawesi Selatan',
       'Sulawesi Tengah', 'Sulawesi Tenggara', 'Sulawesi Utara',
       'Sumatera Barat', 'Sumatera Selatan', 'Sumatera Utara']

X = data.drop(columns=target_columns)
y = data[target_columns]

# Split data into training and testing sets
from sklearn.model_selection import train_test_split # Import the function explicitly
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a multi-output regression model
model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))
model.fit(X_train, y_train)

# Predict
predictions = model.predict(X_test)

# Evaluate model
mape = mean_absolute_percentage_error(y_test, predictions)
print("Mean Absolute Percentage Error per province:", mape)

# Plot actual vs predicted for a sample province
plt.figure(figsize=(10,5))
plt.plot(y_test.iloc[:, 0].values, label='Actual')
plt.plot(predictions[:, 0], label='Predicted')
plt.legend()
plt.title(f"Actual vs Predicted Prices for {target_columns[0]}")
plt.show()

# Load dataset testing
test_data = pd.read_csv("merged_dataset_interpolated1 - merged_dataset_interpolated1 (1).csv")

# Pastikan kolom Date ada dan dalam format datetime
test_data['Date'] = pd.to_datetime(test_data['Date'], dayfirst=True)

test_data.head()

#Extract time-based features
test_data['year'] = test_data['Date'].dt.year
test_data['month'] = test_data['Date'].dt.month
test_data['day'] = test_data['Date'].dt.day
test_data['dayofweek'] = test_data['Date'].dt.dayofweek

# Gunakan hanya fitur waktu untuk prediksi (sesuai dengan model yang telah dilatih)
X_test_new = test_data[X_train.columns]

# Lakukan prediksi
predictions_test = model.predict(X_test_new)

# Simpan hasil prediksi dalam DataFrame
predictions_df = pd.DataFrame(predictions_test, columns=target_columns)
predictions_df.insert(0, 'Date', test_data['Date'])

# Simpan hasil prediksi ke CSV
predictions_df.to_csv("prediksigula.csv", index=False)

# Tampilkan hasil prediksi
print(predictions_df.head())