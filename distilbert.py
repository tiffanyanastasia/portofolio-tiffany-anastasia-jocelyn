# -*- coding: utf-8 -*-
"""distilbert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17vW7RHdW6OX-VLKWnZsCmSm6L5JcUe3j
"""

!pip install -U accelerate>=0.21.0
!pip install -U transformers

!pip show accelerate transformers

from google.colab import drive
drive.mount("/content/drive/")

import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/data4.csv", usecols=["text", "label"])
df.head()

df.shape
df.info()

df.duplicated().sum()

df['text'].str.len().plot.hist(bins=50)

df['label'] = df['label'].str.split(',')

label_counts = [g for gen in df['label'] for g in gen]
pd.Series(label_counts).value_counts()

from sklearn.utils import resample
df_exploded = df.explode('label')

label_counts = df_exploded['label'].value_counts()

cap_value = int(1.5 * label_counts.median())

resampled_dfs = []
for label, count in label_counts.items():
    label_df = df_exploded[df_exploded['label'] == label]
    if count > cap_value:
        resampled_dfs.append(resample(label_df, replace=False, n_samples=cap_value, random_state=42))
    else:
        resampled_dfs.append(label_df)

balanced_df = pd.concat(resampled_dfs)

balanced_label_counts = balanced_df['label'].value_counts()

print(balanced_label_counts)
balanced_df.head()

df = balanced_df.groupby('text')['label'].apply(list).reset_index()

df['label_str'] = df['label'].apply(lambda x: ','.join(x))

df = df.drop_duplicates(subset=['text', 'label_str'])

df = df.drop(columns=['label_str'])

"""Label encoder"""

from sklearn.preprocessing import MultiLabelBinarizer

multilabel = MultiLabelBinarizer()

labels = multilabel.fit_transform(df['label']).astype('float32')

texts = df['text'].tolist()

labels
texts[:5]

"""Model building"""

import torch
from transformers import DistilBertTokenizer, AutoTokenizer
from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset

train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels,
                                                                    test_size=0.2, random_state=42)

checkpoint = "distilbert-base-uncased"
tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)
model = DistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels[0]),
problem_type="multi_label_classification")

labels[0]

# Lets build custom dataset
class CustomDataset(Dataset):
  def __init__(self, texts, labels, tokenizer, max_len=128):
    self.texts = texts
    self.labels = labels
    self.tokenizer = tokenizer
    self.max_len = max_len

  def __len__(self):
    return len(self.texts)

  def __getitem__(self, idx):
    text = str(self.texts[idx])
    label = torch.tensor(self.labels[idx])

    encoding = self.tokenizer(text, truncation=True, padding="max_length", max_length=self.max_len, return_tensors='pt')

    return {
        'input_ids': encoding['input_ids'].flatten(),
        'attention_mask': encoding['attention_mask'].flatten(),
        'labels': label
    }

train_dataset = CustomDataset(train_texts, train_labels, tokenizer)
val_dataset = CustomDataset(val_texts, val_labels, tokenizer)

# val_dataset[0]

import numpy as np
from sklearn.metrics import roc_auc_score, f1_score, hamming_loss, accuracy_score, balanced_accuracy_score
from transformers import EvalPrediction

# Metrics calculation
def multi_labels_metrics(predictions, labels, threshold=0.3):
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(torch.Tensor(predictions))

    y_pred = np.zeros(probs.shape)
    y_pred[np.where(probs >= threshold)] = 1
    y_true = labels

    f1 = f1_score(y_true, y_pred, average='macro')
    roc_auc = roc_auc_score(y_true, y_pred, average='macro')
    hamming = hamming_loss(y_true, y_pred)

    # Calculate balanced accuracy for each label separately and then average them
    balanced_accuracies = []
    for i in range(y_true.shape[1]):
        balanced_accuracies.append(balanced_accuracy_score(y_true[:, i], y_pred[:, i]))
    balanced_acc = np.mean(balanced_accuracies)

    metrics = {
        "roc_auc": roc_auc,
        "hamming_loss": hamming,
        "f1": f1,
        "balanced_accuracy": balanced_acc
    }

    return metrics

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    result = multi_labels_metrics(predictions=preds, labels=p.label_ids)
    return result

pip install transformers[torch]

from transformers import TrainingArguments, Trainer

args = TrainingArguments(
per_device_train_batch_size=8,
per_device_eval_batch_size=8,
output_dir = 'content/drive/MyDrive/BDCresult',
num_train_epochs=5,
save_steps=250,
save_total_limit=2
)

trainer = Trainer(model=model,
args=args,
train_dataset=train_dataset,
eval_dataset = val_dataset,
compute_metrics=compute_metrics)

trainer.train()

trainer.evaluate()

trainer.save_model("hasilbdc")

import pickle
with open("multi-label-binarizer.pkl", "wb") as f:
  pickle.dump(multilabel, f)

!zip -r distilbert.zip "/content/drive/MyDrive/hasilbdc"

"""Prediction"""

text = "Kontrak terbuka Ganjar Pranowo adalah cermin dari kepedulian untuk kita semua. Yuk, bersama jaga kedaulatan pangan!#JNK #GanjarMahfudRebound #GanjarPranowoPilihanUmat https://t.co/SPFC60u1Eg"

encoding = tokenizer(text, return_tensors='pt')
encoding.to(trainer.model.device)

outputs = trainer.model(**encoding)

sigmoid = torch.nn.Sigmoid()
probs = sigmoid(outputs.logits[0].cpu())
preds = np.zeros(probs.shape)
preds[np.where(probs>=0.3)] = 1

multilabel.classes_

multilabel.inverse_transform(preds.reshape(1,-1))

preds.reshape(1,-1)

import pandas as pd
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import numpy as np
import pickle

dataunlabeled = "drive/MyDrive/dataset_unlabeled_penyisihan_bdc_2024.csv"

dataset_unlabeled = pd.read_csv(dataunlabeled, sep=';')

dataset_unlabeled.head()

def predict(text, model, tokenizer, max_len=512):
    encoding = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_len)
    encoding = {k: v.to(model.device) for k, v in encoding.items()}
    outputs = model(**encoding)
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(outputs.logits).detach().cpu().numpy()
    return probs

# Predict labels for each text in the new dataset
predictions = []
for text in dataset_unlabeled['Text']:
    probs = predict(text, model, tokenizer)
    max_prob_idx = np.argmax(probs, axis=1)
    label = multilabel.classes_[max_prob_idx][0]  # Ensure only one label is selected
    predictions.append(label)

# Add predictions to the DataFrame
dataset_unlabeled['Label'] = predictions

# Save the results to a new CSV file
output_path = "/content/drive/MyDrive/prediksibdc.csv"
dataset_unlabeled.to_csv(output_path, index=False)

print(f"Results saved to {output_path}")

"""predict dataset"""